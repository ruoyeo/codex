# Codex CLI Configuration
# This configuration uses LiteLLM as the model provider gateway
# See: https://docs.litellm.ai/docs/simple_proxy
model = "gpt-5"
model_provider = "github"
model_reasoning_summary = "detailed"
show_raw_agent_reasoning = true
model_verbosity = "medium"
approval_policy = "on-request"
model_reasoning_effort = "high"

# Model providers (LiteLLM as Github Copilot proxy)
[model_providers.github]
name     = "OpenAI"
base_url = "http://localhost:4000"
http_headers = { "Authorization"= "Bearer sk-dummy"}
wire_api = "chat"

# MCP Tools
[mcp_servers.context7]
command = "npx"
args = ["-y", "@upstash/context7-mcp@latest"]

[mcp_servers.claude]
command = "claude"
args = ["mcp", "serve"]
